{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Bayesian Optimisation Code\n",
    "# --------------------------\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern,RBF\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern,RBF\n",
    "from scipy.optimize import minimize\n",
    "from pyDOE import *\n",
    "\n",
    "# -----------------------------------------\n",
    "# --- Class for a continuous hyperparameter\n",
    "# -----------------------------------------\n",
    "    \n",
    "# --- Define a hyperparameter class that contains all the required specs of the hyperparameter\n",
    "class hyperparam(object):\n",
    "    \n",
    "    def __init__(self,list_in):\n",
    "        \n",
    "        # Initiate with 2 types of variable. We either specify bounds\n",
    "        # for continuous variable or values for discrete. Note that for\n",
    "        # now the values must be integers and be a list of consecutive\n",
    "        #Â integers.\n",
    "        if len(list_in) == 2:\n",
    "            self.bounds = list_in\n",
    "            self.kind = 'continuous'\n",
    "        elif len(list_in) > 2:\n",
    "            self.bounds = [list_in[0],list_in[-1]]\n",
    "            self.kind = 'discrete'\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class iteration(object):\n",
    "    \n",
    "    def __init__(self,pars):\n",
    "        \n",
    "        \n",
    "#         # --- Sample data\n",
    "        self.Xt = pars.Xt\n",
    "        self.Yt = pars.Yt\n",
    "       \n",
    "        # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "        X_next = self.propose_location(pars)\n",
    "        # Convert to int where necessary\n",
    "        \n",
    "        # We need to recreate a dictionary with the keys given by the hyperparameter name before pasing into our\n",
    "        # ML model\n",
    "        self.X_nextdict = {}\n",
    "        for i,hps1 in enumerate(sorted(pars.Xtdict.keys())):\n",
    "            if pars.hps[hps1].kind == 'discrete':\n",
    "                X_next[i] = int(X_next[i])\n",
    "                self.X_nextdict[hps1] = X_next[i]\n",
    "            else:\n",
    "                self.X_nextdict[hps1] = X_next[i]\n",
    "        \n",
    "        #X_next = np.array(X_next,ndmin=(2)).reshape(1,-1)\n",
    "        Y_next = pars.objF(self.X_nextdict)\n",
    "        \n",
    "        # Add the new sample point to the existing for the next iteration\n",
    "        self.Xt = np.vstack((self.Xt, X_next.reshape(1,-1)[0]))\n",
    "        self.Yt = np.concatenate((self.Yt, Y_next))\n",
    "    \n",
    "    # Sampling function to find the next values for the hyperparameters\n",
    "    def propose_location(self,pars):\n",
    "        \n",
    "        # Proposes the next sampling point by optimizing the acquisition function. Args: acquisition: Acquisition function. X_sample: Sample locations (n x d). Y_sample: Sample values (n x 1). gpr: A GaussianProcessRegressor fitted to samples. Returns: Location of the acquisition function maximum. '''\n",
    "        self.N_hps = pars.Xt.shape[1]\n",
    "        min_val = 1\n",
    "        min_x = None\n",
    "\n",
    "        self.gpr = pars.gpr\n",
    "        self.Xt = pars.Xt\n",
    "    \n",
    "\n",
    "        # Find the best optimum by starting from n_restart different random points.\n",
    "        Xs = lhs(self.N_hps, samples=pars.n_restarts, criterion='centermaximin')\n",
    "        for i,hp in enumerate(sorted(pars.hps.keys())):\n",
    "            Xs[:,i] = Xs[:,i]*(pars.hps[hp].bounds[1]-pars.hps[hp].bounds[0])+pars.hps[hp].bounds[0]\n",
    "        \n",
    "            # Convert int values to integers\n",
    "            if pars.hps[hp].kind == 'discrete':\n",
    "                Xs[:,i] = Xs[:,i].astype(int)\n",
    "        \n",
    "        for x0 in Xs:\n",
    "            res = minimize(self.min_obj, x0=x0, bounds=pars.bounds, method=pars.method) \n",
    "            # Find the best optimum across all initiations\n",
    "            if res.fun < min_val:\n",
    "                min_val = res.fun[0]\n",
    "                min_x = res.x           \n",
    "\n",
    "        return min_x.reshape(-1, 1)\n",
    "    \n",
    "    def min_obj(self,X):\n",
    "    # Minimization objective is the negative acquisition function\n",
    "        return -self.expected_improvement(X.reshape(-1, self.N_hps))\n",
    "        \n",
    "    # Acquisition function - here we use expected improvement\n",
    "    def expected_improvement(self,X):\n",
    "        \n",
    "        # --- Computes the EI at points X based on existing samples X_sample and Y_sample using a Gaussian process \n",
    "        # surrogate model. \n",
    "        # X: Points at which EI shall be computed (m x d). \n",
    "        # X_sample: Sample locations (n x d). \n",
    "        # Y_sample: Sample values (n x 1). \n",
    "        # gpr: A GaussianProcessRegressor fitted to samples. \n",
    "        # xi: Exploitation-exploration trade-off parameter. \n",
    "        #.   - xi ~ O(0) => exploitation\n",
    "        #.   - xi ~ O(1) => exploration\n",
    "        # Returns: Expected improvements at points X.\n",
    "\n",
    "        # Evaluate the Gaussian Process at a test location X to get the mean and std\n",
    "        mu, sigma = self.gpr.predict(X, return_std=True)\n",
    "        # Evaluate the Gaussian Process at the sampled points - this gets the mean values without the noise\n",
    "        mu_sample = self.gpr.predict(self.Xt)\n",
    "\n",
    "        \n",
    "        sigma = sigma.reshape(-1, 1)#self.Xt.shape[1])\n",
    "\n",
    "        # Needed for noise-based model,\n",
    "        # otherwise use np.max(Y_sample).\n",
    "        # See also section 2.4 in [...]\n",
    "        mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "        imp = mu - mu_sample_opt\n",
    "        Z = imp / sigma\n",
    "\n",
    "        Ei = (mu-mu_sample_opt) * norm.cdf(mu,loc=mu_sample_opt, scale=sigma) \\\n",
    "            + mu_sample_opt * norm.pdf(mu,loc=mu_sample_opt, scale=sigma)\n",
    "\n",
    "\n",
    "        return Ei\n",
    "    \n",
    "    \n",
    "    \n",
    "class BayesianOptimisation(object):\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        \n",
    "        # Get hyperparameter info and convert to hyperparameter class\n",
    "        self.hps = {}\n",
    "        for hp in kwargs['hps'].keys():\n",
    "            self.hps[hp] = hyperparam(kwargs['hps'][hp])\n",
    "        \n",
    "        # Objective function to minimise\n",
    "        self.MLmodel = kwargs['MLmodel']\n",
    "        \n",
    "            \n",
    "        # Number of hyperparameters\n",
    "        N_hps = len(self.hps.keys())\n",
    "        \n",
    "        # --- Initial sample data\n",
    "        if 'NpI' in kwargs.keys():\n",
    "            self.NpI = kwargs['NpI']\n",
    "        else:\n",
    "            self.NpI = 2**N_hps\n",
    "        \n",
    "        \n",
    "        # Establish a dictionary for our hyperparameter values that we sample\n",
    "        self.Xtdict = {}\n",
    "        # ...and then an array for the same thing but with each column being\n",
    "        # a different hyperparameter and ordered alphabetically\n",
    "        self.Xt = np.zeros((self.NpI,len(self.hps.keys())))\n",
    "        # We also need to collect together all of the bounds for the optimization routing into one array\n",
    "        self.bounds = np.zeros((len(self.hps.keys()),2))\n",
    "        \n",
    "        # Get some initial samples on the unit interval\n",
    "        Xt = lhs(len(self.hps.keys()), samples=self.NpI, criterion='centermaximin')\n",
    "        \n",
    "        # For each hyper parameter, rescale the unit inverval on the \n",
    "        # appropriate range for that hp and store in a dict\n",
    "        for i,hp in enumerate(sorted(self.hps.keys())):\n",
    "            self.Xtdict[hp] = self.hps[hp].bounds[0]+Xt[:,i]*(self.hps[hp].bounds[1]-self.hps[hp].bounds[0])\n",
    "            # convert these to an int if kind = 'discrete'\n",
    "            \n",
    "            if self.hps[hp].kind == 'discrete':\n",
    "                self.Xtdict[hp] = self.Xtdict[hp].astype(int)\n",
    "            \n",
    "            self.bounds[i,:] = self.hps[hp].bounds\n",
    "\n",
    "            self.Xt[:,i] = self.Xtdict[hp]\n",
    "    \n",
    "\n",
    "        \n",
    "            \n",
    "        # Calculate objective function at the sampled points\n",
    "        self.Yt = self.objF(pars=self.Xtdict,n=self.NpI)\n",
    "\n",
    "        \n",
    "        \n",
    "        # --- Number of iterations\n",
    "        if 'Niter' in kwargs.keys():\n",
    "            self.Niter = kwargs['Niter']\n",
    "        else:\n",
    "            self.Niter = 10*N_hps\n",
    "            \n",
    "        # --- Number of optimisations of the acquisition function\n",
    "        if 'n_restarts' in kwargs.keys():\n",
    "            self.n_restarts = kwargs['n_restarts']\n",
    "        else:\n",
    "            self.n_restarts = 25*N_hps\n",
    "            \n",
    "        # --- Optimisation method used\n",
    "        if 'method' in kwargs.keys():\n",
    "            self.method = kwargs['method']\n",
    "        else:\n",
    "            self.method = 'L-BFGS-B'\n",
    "            \n",
    "        \n",
    "        # --- Define the Gaussian mixture model\n",
    "        if 'kernel' in kwargs.keys():\n",
    "            self.kernel = kwargs['kernel']\n",
    "        else:\n",
    "            self.kernel = RBF()\n",
    "            \n",
    "        if 'noise' in kwargs.keys():\n",
    "            self.noise = kwargs['noise']\n",
    "        else:\n",
    "            self.noise = noise = 0.2\n",
    "            \n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise**2)\n",
    "        \n",
    "    def optimise(self):\n",
    "        for i in range(self.Niter):\n",
    "            it1 = iteration(self)\n",
    "            self.Xt = it1.Xt\n",
    "            self.Yt = it1.Yt\n",
    "            print('current accuracy:',self.Yt[-1])\n",
    "            print('best accuracy:', max(self.Yt))\n",
    "            self.gpr.fit(self.Xt, self.Yt)\n",
    "        return self\n",
    "    \n",
    "    def objF(self,pars,**kwargs):\n",
    "        \n",
    "        # Number of hyperparameter values to try. \n",
    "        n = 1\n",
    "        if 'n' in kwargs.keys():\n",
    "            n = kwargs['n']\n",
    "\n",
    "        # Initiate array to accumate the accuracy of the model\n",
    "        sc = np.zeros(n)\n",
    "        \n",
    "        # Establish the basic ML model\n",
    "        model = self.MLmodel\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            # Get dictionary of hyperparameter values to test at the ith iteration\n",
    "            hps_iter = {}\n",
    "            for hp in pars.keys():\n",
    "                if self.hps[hp].kind == 'discrete':\n",
    "                    hps_iter[hp] = int(pars[hp][i])\n",
    "                else:\n",
    "                    hps_iter[hp] = pars[hp][i]\n",
    "                \n",
    "            # Create instance of MLmodel with the hps at this iteration\n",
    "            model.set_params(**hps_iter)\n",
    "        \n",
    "            # Train\n",
    "            model.fit(X_train,y_train)\n",
    "            \n",
    "            # Score\n",
    "            sc[i] = np.mean(cross_val_score(model, X_train,y_train, cv=5))\n",
    "            \n",
    "        return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BayesianOptimisation at 0x105d43ef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train = np.zeros((100,8))\n",
    "y_train = np.ones((100,))\n",
    "\n",
    "hps_rf = {\n",
    "    'n_estimators':range(10,21),\n",
    "    'max_depth':range(1,10)\n",
    "}\n",
    "BayesianOptimisation(\n",
    "    hps=hps_rf,\n",
    "    MLmodel = RandomForestRegressor()\n",
    ").optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
