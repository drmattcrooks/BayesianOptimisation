{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalised model set up that allows passing of arbitrary model for fitting, \n",
    "# hyperparameters can specified as values = [_v1,_v2,...,_vn] for discrete and bounds = [_lower,_upper] for continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Bayesian Optimisation Code\n",
    "# --------------------------\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern,RBF\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern,RBF\n",
    "from scipy.optimize import minimize\n",
    "from pyDOE import *\n",
    "\n",
    "# -----------------------------------------\n",
    "# --- Class for a continuous hyperparameter\n",
    "# -----------------------------------------\n",
    "    \n",
    "# --- Define a hyperparameter class that contains all the required specs of the hyperparameter\n",
    "class hyperparam(object):\n",
    "    \n",
    "    def __init__(self,list_in):\n",
    "        \n",
    "        # Initiate with 2 types of variable. We either specify bounds\n",
    "        # for continuous variable or values for discrete. Note that for\n",
    "        # now the values must be integers and be a list of consecutive\n",
    "        #Â integers.\n",
    "        if len(list_in) == 2:\n",
    "            self.bounds = list_in\n",
    "            self.kind = 'continuous'\n",
    "        elif len(list_in) > 2:\n",
    "            self.bounds = [list_in[0],list_in[-1]]\n",
    "            self.kind = 'discrete'\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class iteration(object):\n",
    "    \n",
    "    def __init__(self,pars):\n",
    "        \n",
    "        \n",
    "#         # --- Sample data\n",
    "        self.Xt = pars.Xt\n",
    "        self.Yt = pars.Yt\n",
    "       \n",
    "        # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "        X_next = self.propose_location(pars)\n",
    "        # Convert to int where necessary\n",
    "        \n",
    "        # We need to recreate a dictionary with the keys given by the hyperparameter name before pasing into our\n",
    "        # ML model\n",
    "        self.X_nextdict = {}\n",
    "        for i,hps1 in enumerate(sorted(pars.Xtdict.keys())):\n",
    "            if pars.hps[hps1].kind == 'discrete':\n",
    "                X_next[i] = int(X_next[i])\n",
    "                self.X_nextdict[hps1] = X_next[i]\n",
    "            else:\n",
    "                self.X_nextdict[hps1] = X_next[i]\n",
    "        \n",
    "        #X_next = np.array(X_next,ndmin=(2)).reshape(1,-1)\n",
    "        Y_next = pars.objF(self.X_nextdict)\n",
    "        \n",
    "        # Add the new sample point to the existing for the next iteration\n",
    "        self.Xt = np.vstack((self.Xt, X_next.reshape(1,-1)[0]))\n",
    "        self.Yt = np.concatenate((self.Yt, Y_next))\n",
    "    \n",
    "    # Sampling function to find the next values for the hyperparameters\n",
    "    def propose_location(self,pars):\n",
    "        \n",
    "        # Proposes the next sampling point by optimizing the acquisition function. Args: acquisition: Acquisition function. X_sample: Sample locations (n x d). Y_sample: Sample values (n x 1). gpr: A GaussianProcessRegressor fitted to samples. Returns: Location of the acquisition function maximum. '''\n",
    "        self.N_hps = pars.Xt.shape[1]\n",
    "        min_val = 1\n",
    "        min_x = None\n",
    "\n",
    "        self.gpr = pars.gpr\n",
    "        self.Xt = pars.Xt\n",
    "    \n",
    "\n",
    "        # Find the best optimum by starting from n_restart different random points.\n",
    "        Xs = lhs(self.N_hps, samples=pars.n_restarts, criterion='centermaximin')\n",
    "        for i,hp in enumerate(sorted(pars.hps.keys())):\n",
    "            Xs[:,i] = Xs[:,i]*(pars.hps[hp].bounds[1]-pars.hps[hp].bounds[0])+pars.hps[hp].bounds[0]\n",
    "        \n",
    "            # Convert int values to integers\n",
    "            if pars.hps[hp].kind == 'discrete':\n",
    "                Xs[:,i] = Xs[:,i].astype(int)\n",
    "        \n",
    "        for x0 in Xs:\n",
    "            res = minimize(self.min_obj, x0=x0, bounds=pars.bounds, method=pars.method) \n",
    "            # Find the best optimum across all initiations\n",
    "            if res.fun < min_val:\n",
    "                min_val = res.fun[0]\n",
    "                min_x = res.x           \n",
    "\n",
    "        return min_x.reshape(-1, 1)\n",
    "    \n",
    "    def min_obj(self,X):\n",
    "    # Minimization objective is the negative acquisition function\n",
    "        return -self.expected_improvement(X.reshape(-1, self.N_hps))\n",
    "        \n",
    "    # Acquisition function - here we use expected improvement\n",
    "    def expected_improvement(self,X):\n",
    "        \n",
    "        # --- Computes the EI at points X based on existing samples X_sample and Y_sample using a Gaussian process \n",
    "        # surrogate model. \n",
    "        # X: Points at which EI shall be computed (m x d). \n",
    "        # X_sample: Sample locations (n x d). \n",
    "        # Y_sample: Sample values (n x 1). \n",
    "        # gpr: A GaussianProcessRegressor fitted to samples. \n",
    "        # xi: Exploitation-exploration trade-off parameter. \n",
    "        #.   - xi ~ O(0) => exploitation\n",
    "        #.   - xi ~ O(1) => exploration\n",
    "        # Returns: Expected improvements at points X.\n",
    "\n",
    "        # Evaluate the Gaussian Process at a test location X to get the mean and std\n",
    "        mu, sigma = self.gpr.predict(X, return_std=True)\n",
    "        # Evaluate the Gaussian Process at the sampled points - this gets the mean values without the noise\n",
    "        mu_sample = self.gpr.predict(self.Xt)\n",
    "\n",
    "        \n",
    "        sigma = sigma.reshape(-1, 1)#self.Xt.shape[1])\n",
    "\n",
    "        # Needed for noise-based model,\n",
    "        # otherwise use np.max(Y_sample).\n",
    "        # See also section 2.4 in [...]\n",
    "        mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "        imp = mu - mu_sample_opt\n",
    "        Z = imp / sigma\n",
    "\n",
    "        Ei = (mu-mu_sample_opt) * norm.cdf(mu,loc=mu_sample_opt, scale=sigma) \\\n",
    "            + mu_sample_opt * norm.pdf(mu,loc=mu_sample_opt, scale=sigma)\n",
    "\n",
    "\n",
    "        return Ei\n",
    "    \n",
    "    \n",
    "    \n",
    "class BayesianOptimisation(object):\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        \n",
    "        # Get hyperparameter info and convert to hyperparameter class\n",
    "        self.hps = {}\n",
    "        for hp in kwargs['hps'].keys():\n",
    "            self.hps[hp] = hyperparam(kwargs['hps'][hp])\n",
    "        \n",
    "        # Objective function to minimise\n",
    "        self.MLmodel = kwargs['MLmodel']\n",
    "        \n",
    "            \n",
    "        # Number of hyperparameters\n",
    "        N_hps = len(self.hps.keys())\n",
    "        \n",
    "        # --- Initial sample data\n",
    "        if 'NpI' in kwargs.keys():\n",
    "            self.NpI = kwargs['NpI']\n",
    "        else:\n",
    "            self.NpI = 2**N_hps\n",
    "        \n",
    "        \n",
    "        # Establish a dictionary for our hyperparameter values that we sample\n",
    "        self.Xtdict = {}\n",
    "        # ...and then an array for the same thing but with each column being\n",
    "        # a different hyperparameter and ordered alphabetically\n",
    "        self.Xt = np.zeros((self.NpI,len(self.hps.keys())))\n",
    "        # We also need to collect together all of the bounds for the optimization routing into one array\n",
    "        self.bounds = np.zeros((2,len(self.hps.keys())))\n",
    "        \n",
    "        # Get some initial samples on the unit interval\n",
    "        Xt = lhs(len(self.hps.keys()), samples=self.NpI, criterion='centermaximin')\n",
    "        \n",
    "        # For each hyper parameter, rescale the unit inverval on the \n",
    "        # appropriate range for that hp and store in a dict\n",
    "        for i,hp in enumerate(sorted(self.hps.keys())):\n",
    "            self.Xtdict[hp] = self.hps[hp].bounds[0]+Xt[:,i]*(self.hps[hp].bounds[1]-self.hps[hp].bounds[0])\n",
    "            # convert these to an int if kind = 'discrete'\n",
    "            \n",
    "            if self.hps[hp].kind == 'discrete':\n",
    "                self.Xtdict[hp] = self.Xtdict[hp].astype(int)\n",
    "            \n",
    "            self.bounds[i,:] = self.hps[hp].bounds\n",
    "\n",
    "            self.Xt[:,i] = self.Xtdict[hp]\n",
    "    \n",
    "\n",
    "        \n",
    "            \n",
    "        # Calculate objective function at the sampled points\n",
    "        self.Yt = self.objF(pars=self.Xtdict,n=self.NpI)\n",
    "\n",
    "        \n",
    "        \n",
    "        # --- Number of iterations\n",
    "        if 'Niter' in kwargs.keys():\n",
    "            self.Niter = kwargs['Niter']\n",
    "        else:\n",
    "            self.Niter = 10*N_hps\n",
    "            \n",
    "        # --- Number of optimisations of the acquisition function\n",
    "        if 'n_restarts' in kwargs.keys():\n",
    "            self.n_restarts = kwargs['n_restarts']\n",
    "        else:\n",
    "            self.n_restarts = 25*N_hps\n",
    "            \n",
    "        # --- Optimisation method used\n",
    "        if 'method' in kwargs.keys():\n",
    "            self.method = kwargs['method']\n",
    "        else:\n",
    "            self.method = 'L-BFGS-B'\n",
    "            \n",
    "        \n",
    "        # --- Define the Gaussian mixture model\n",
    "        if 'kernel' in kwargs.keys():\n",
    "            self.kernel = kwargs['kernel']\n",
    "        else:\n",
    "            self.kernel = RBF()\n",
    "            \n",
    "        if 'noise' in kwargs.keys():\n",
    "            self.noise = kwargs['noise']\n",
    "        else:\n",
    "            self.noise = noise = 0.2\n",
    "            \n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=noise**2)\n",
    "        \n",
    "    def optimise(self):\n",
    "        for i in range(self.Niter):\n",
    "            it1 = iteration(self)\n",
    "            self.Xt = it1.Xt\n",
    "            self.Yt = it1.Yt\n",
    "            print('current accuracy:',self.Yt[-1])\n",
    "            print('best accuracy:', max(self.Yt))\n",
    "            self.gpr.fit(self.Xt, self.Yt)\n",
    "        return self\n",
    "    \n",
    "    def objF(self,pars,**kwargs):\n",
    "        \n",
    "        # Number of hyperparameter values to try. \n",
    "        n = 1\n",
    "        if 'n' in kwargs.keys():\n",
    "            n = kwargs['n']\n",
    "\n",
    "        # Initiate array to accumate the accuracy of the model\n",
    "        sc = np.zeros(n)\n",
    "        \n",
    "        # Establish the basic ML model\n",
    "        model = self.MLmodel\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            # Get dictionary of hyperparameter values to test at the ith iteration\n",
    "            hps_iter = {}\n",
    "            for hp in pars.keys():\n",
    "                if self.hps[hp].kind == 'discrete':\n",
    "                    hps_iter[hp] = int(pars[hp][i])\n",
    "                else:\n",
    "                    hps_iter[hp] = pars[hp][i]\n",
    "                \n",
    "            # Create instance of MLmodel with the hps at this iteration\n",
    "            model.set_params(**hps_iter)\n",
    "        \n",
    "            # Train\n",
    "            model.fit(X_train,y_train)\n",
    "            \n",
    "            # Score\n",
    "            sc[i] = np.mean(cross_val_score(model, X_train,y_train, cv=5))\n",
    "            \n",
    "        return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n",
      "current accuracy: 1.0\n",
      "best accuracy: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3b6bae580be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m BayesianOptimisation(\n\u001b[1;32m     12\u001b[0m     \u001b[0mhps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhps_rf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mMLmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m ).optimise()\n",
      "\u001b[0;32m<ipython-input-3-4e5a6ac7f975>\u001b[0m in \u001b[0;36moptimise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mit1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e5a6ac7f975>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pars)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Obtain next sampling point from the acquisition function (expected_improvement)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropose_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Convert to int where necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e5a6ac7f975>\u001b[0m in \u001b[0;36mpropose_location\u001b[0;34m(self, pars)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Find the best optimum across all initiations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 601\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e5a6ac7f975>\u001b[0m in \u001b[0;36mmin_obj\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmin_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Minimization objective is the negative acquisition function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Acquisition function - here we use expected improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e5a6ac7f975>\u001b[0m in \u001b[0;36mexpected_improvement\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Evaluate the Gaussian Process at the sampled points - this gets the mean values without the noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mmu_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \"returning full covariance.\")\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_train_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unfitted;predict based on GP prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m# thereby passing the test made in the lines following the scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# of warnings context manager.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/warnings.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters_mutated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowwarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train = np.zeros((100,8))\n",
    "y_train = np.ones((100,))\n",
    "\n",
    "hps_rf = {\n",
    "    'n_estimators':range(10,21),\n",
    "    'max_depth':range(1,10)\n",
    "}\n",
    "BayesianOptimisation(\n",
    "    hps=hps_rf,\n",
    "    MLmodel = RandomForestRegressor()\n",
    ").optimise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
